{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/erberry/ThinkML/blob/main/convert_and_quantize_chinese_llama_and_alpaca.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "FekGsgTJHCeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# è½¬æ¢å¹¶é‡åŒ–ä¸­æ–‡LLaMAå’ŒAlpacaæ¨¡å‹\n",
        "\n",
        "é¡¹ç›®åœ°å€ï¼šhttps://github.com/ymcui/Chinese-LLaMA-Alpaca\n",
        "\n",
        "âš ï¸ å†…å­˜æ¶ˆè€—æç¤ºï¼ˆç¡®ä¿åˆ·å‡ºæ¥çš„æœºå™¨RAMå¤§äºä»¥ä¸‹è¦æ±‚ï¼‰ï¼š\n",
        "- 7Bæ¨¡å‹ï¼š15G+\n",
        "- 13Bæ¨¡å‹ï¼š18G+\n",
        "- 33Bæ¨¡å‹ï¼š22G+\n",
        "\n",
        "ğŸ’¡ æç¤ºå’Œå°çªé—¨ï¼š\n",
        "- å…è´¹ç”¨æˆ·é»˜è®¤çš„å†…å­˜åªæœ‰12Gå·¦å³ï¼Œä¸è¶³ä»¥è½¬æ¢æ¨¡å‹ã€‚**å®æµ‹é€‰æ‹©TPUçš„è¯æœ‰æœºä¼šéšæœºå‡º35Gå†…å­˜**ï¼Œå»ºè®®å¤šè¯•å‡ æ¬¡\n",
        "- Pro(+)ç”¨æˆ·è¯·é€‰æ‹© â€œä»£ç æ‰§è¡Œç¨‹åºâ€ -> â€œæ›´æ”¹è¿è¡Œæ—¶ç±»å‹â€ -> â€œé«˜RAMâ€\n",
        "- ç¨‹åºè«åå´©æ‰æˆ–æ–­å¼€è¿æ¥å°±è¯´æ˜å†…å­˜çˆ†äº†\n",
        "- å¦‚æœé€‰äº†â€œé«˜RAMâ€ä¹‹åå†…å­˜è¿˜æ˜¯ä¸å¤Ÿå¤§çš„è¯ï¼Œé€‰æ‹©ä»¥ä¸‹æ“ä½œï¼Œæœ‰çš„æ—¶å€™ä¼šåˆ†é…å‡ºå¾ˆé«˜å†…å­˜çš„æœºå™¨ï¼Œç¥ä½ å¥½è¿ğŸ˜„ï¼\n",
        "    - å¯ä»¥æŠŠGPUæˆ–è€…TPUä¹Ÿé€‰ä¸Šï¼ˆè™½ç„¶ä¸ä¼šç”¨åˆ°ï¼‰\n",
        "    - é€‰GPUæ—¶ï¼ŒPro(+)ç”¨æˆ·å¯é€‰â€œA100â€ç±»å‹GPU\n",
        "\n",
        "*æ¸©é¦¨æç¤ºï¼šç”¨å®Œä¹‹åæ³¨æ„æ–­å¼€è¿è¡Œæ—¶ï¼Œé€‰æ‹©æ»¡è¶³è¦æ±‚çš„æœ€ä½é…ç½®å³å¯ï¼Œé¿å…ä¸å¿…è¦çš„è®¡ç®—å•å…ƒæ¶ˆè€—ï¼ˆProåªç»™100ä¸ªè®¡ç®—å•å…ƒï¼‰ã€‚*"
      ],
      "metadata": {
        "id": "B1c96_k3MahN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## å®‰è£…ç›¸å…³ä¾èµ–"
      ],
      "metadata": {
        "id": "vScqHD_jMFOV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5WKFJXIL6ZU"
      },
      "outputs": [],
      "source": [
        "!pip install torch==1.13.1\n",
        "!pip install transformers==4.30.2\n",
        "!pip install peft==0.3.0\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## å…‹éš†ç›®å½•å’Œä»£ç "
      ],
      "metadata": {
        "id": "ygb1xFIMNQKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ymcui/Chinese-LLaMA-Alpaca\n",
        "!git clone https://github.com/ggerganov/llama.cpp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCEJh7NJNXz9",
        "outputId": "abc18bc9-82be-4348-9bb1-36258ee40bf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Chinese-LLaMA-Alpaca'...\n",
            "remote: Enumerating objects: 1927, done.\u001b[K\n",
            "remote: Counting objects: 100% (496/496), done.\u001b[K\n",
            "remote: Compressing objects: 100% (182/182), done.\u001b[K\n",
            "remote: Total 1927 (delta 340), reused 416 (delta 314), pack-reused 1431\u001b[K\n",
            "Receiving objects: 100% (1927/1927), 22.93 MiB | 30.25 MiB/s, done.\n",
            "Resolving deltas: 100% (1159/1159), done.\n",
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 4187, done.\u001b[K\n",
            "remote: Counting objects: 100% (1772/1772), done.\u001b[K\n",
            "remote: Compressing objects: 100% (192/192), done.\u001b[K\n",
            "remote: Total 4187 (delta 1677), reused 1599 (delta 1580), pack-reused 2415\u001b[K\n",
            "Receiving objects: 100% (4187/4187), 3.63 MiB | 23.07 MiB/s, done.\n",
            "Resolving deltas: 100% (2833/2833), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## åˆå¹¶æ¨¡å‹ï¼ˆä»¥Alpaca-13Bä¸ºä¾‹ï¼‰\n",
        "\n",
        "æ­¤å¤„ä½¿ç”¨çš„æ˜¯ğŸ¤—æ¨¡å‹åº“ä¸­æä¾›çš„åŸºæ¨¡å‹ï¼ˆå·²æ˜¯HFæ ¼å¼ï¼‰ï¼Œè€Œä¸æ˜¯Facebookå®˜æ–¹çš„LLaMAæ¨¡å‹ï¼Œå› æ­¤ç•¥å»å°†åŸç‰ˆLLaMAè½¬æ¢ä¸ºHFæ ¼å¼çš„æ­¥éª¤ã€‚\n",
        "**è¿™é‡Œç›´æ¥è¿è¡Œç¬¬äºŒæ­¥ï¼šåˆå¹¶LoRAæƒé‡**ï¼Œç”Ÿæˆå…¨é‡æ¨¡å‹æƒé‡ã€‚å¯ä»¥ç›´æ¥æŒ‡å®šğŸ¤—æ¨¡å‹åº“çš„åœ°å€ï¼Œä¹Ÿå¯ä»¥æ˜¯æœ¬åœ°å­˜æ”¾åœ°å€ã€‚\n",
        "- åŸºæ¨¡å‹ï¼š`elinas/llama-13b-hf-transformers-4.29` *ï¼ˆuse at your own riskï¼Œæˆ‘ä»¬æ¯”å¯¹è¿‡SHA256å’Œæ­£ç‰ˆä¸€è‡´ï¼Œä½†ä½ åº”ç¡®ä¿è‡ªå·±æœ‰æƒä½¿ç”¨è¯¥æ¨¡å‹ï¼‰*\n",
        "- LoRAæ¨¡å‹ï¼š`ziqingyang/chinese-alpaca-lora-13b`\n",
        "   - å¦‚æœæ˜¯Alpaca-Plusæ¨¡å‹ï¼Œè®°å¾—è¦åŒæ—¶ä¼ å…¥llamaå’Œalpacaçš„loraï¼Œæ•™ç¨‹ï¼š[è¿™é‡Œ](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/æ‰‹åŠ¨æ¨¡å‹åˆå¹¶ä¸è½¬æ¢#å¤šloraæƒé‡åˆå¹¶é€‚ç”¨äºchinese-alpaca-plus)\n",
        "- è¾“å‡ºæ ¼å¼ï¼šå¯é€‰pthæˆ–è€…huggingfaceï¼Œè¿™é‡Œé€‰æ‹©pthï¼Œå› ä¸ºåé¢è¦ç”¨llama.cppé‡åŒ–\n",
        "\n",
        "ç”±äºè¦ä¸‹è½½æ¨¡å‹ï¼Œæ‰€ä»¥éœ€è¦è€å¿ƒç­‰å¾…ä¸€ä¸‹ï¼Œå°¤å…¶æ˜¯33Bæ¨¡å‹ã€‚\n",
        "è½¬æ¢å¥½çš„æ¨¡å‹å­˜æ”¾åœ¨`alpaca-combined`ç›®å½•ã€‚\n",
        "å¦‚æœä½ ä¸éœ€è¦é‡åŒ–æ¨¡å‹ï¼Œé‚£ä¹ˆåˆ°è¿™ä¸€æ­¥å°±ç»“æŸäº†ï¼Œå¯è‡ªè¡Œä¸‹è½½æˆ–è€…è½¬å­˜åˆ°Google Driveã€‚"
      ],
      "metadata": {
        "id": "nIyxX0DSNsgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python ./Chinese-LLaMA-Alpaca/scripts/merge_llama_with_chinese_lora_low_mem.py \\\n",
        "    --base_model 'elinas/llama-13b-hf-transformers-4.29' \\\n",
        "    --lora_model 'ziqingyang/chinese-alpaca-lora-13b' \\\n",
        "    --output_type pth \\\n",
        "    --output_dir alpaca-combined"
      ],
      "metadata": {
        "id": "5AV4EW5hNhVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## æ¯”å¯¹SHA256\n",
        "\n",
        "å®Œæ•´å€¼ï¼šhttps://github.com/ymcui/Chinese-LLaMA-Alpaca/blob/main/SHA256.md\n",
        "\n",
        "å…¶ä¸­æœ¬ç¤ºä¾‹ç”Ÿæˆçš„Alpaca-13Bçš„æ ‡å‡†SHA256ï¼š\n",
        "- 30cefb5be9091c3e17fbba5d91bf16266a2ddf86cde53370a9982b232ff8a2f4\n",
        "- ce946742b0f122f472e192c3f77d506e0c26578b4b881d07d919553333affecd\n",
        "\n",
        "ä½¿ç”¨ä¸‹è¿°å‘½ä»¤è¯„æµ‹åå‘ç°ä¸¤è€…ç›¸åŒï¼Œåˆå¹¶æ— è¯¯ã€‚"
      ],
      "metadata": {
        "id": "iO6f_kZOPB_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sha256sum alpaca-combined/consolidated.*.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5u4QDNZPYI_",
        "outputId": "fa94adca-0363-46a7-8879-908272d7582d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30cefb5be9091c3e17fbba5d91bf16266a2ddf86cde53370a9982b232ff8a2f4  alpaca-combined/consolidated.00.pth\n",
            "ce946742b0f122f472e192c3f77d506e0c26578b4b881d07d919553333affecd  alpaca-combined/consolidated.01.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## é‡åŒ–æ¨¡å‹\n",
        "æ¥ä¸‹æ¥æˆ‘ä»¬ä½¿ç”¨[llama.cpp](https://github.com/ggerganov/llama.cpp)å·¥å…·å¯¹ä¸Šä¸€æ­¥ç”Ÿæˆçš„å…¨é‡ç‰ˆæœ¬æƒé‡è¿›è¡Œè½¬æ¢ï¼Œç”Ÿæˆ4-bité‡åŒ–æ¨¡å‹ã€‚\n",
        "\n",
        "### ç¼–è¯‘å·¥å…·\n",
        "\n",
        "é¦–å…ˆå¯¹llama.cppå·¥å…·è¿›è¡Œç¼–è¯‘ã€‚"
      ],
      "metadata": {
        "id": "ueexcKo-Q_EW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd llama.cpp && make"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GbjsT2wRRCR",
        "outputId": "42bd07bc-9db7-4547-e176-b9cb4e9dca5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I llama.cpp build info: \n",
            "I UNAME_S:  Linux\n",
            "I UNAME_P:  x86_64\n",
            "I UNAME_M:  x86_64\n",
            "I CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS\n",
            "I CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS\n",
            "I LDFLAGS:  \n",
            "I CC:       cc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\n",
            "I CXX:      g++ (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\n",
            "\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -c llama.cpp -o llama.o\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/main/main.cpp ggml.o llama.o common.o k_quants.o -o main \n",
            "\n",
            "====  Run ./main -h for help.  ====\n",
            "\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/quantize/quantize.cpp ggml.o llama.o k_quants.o -o quantize \n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/quantize-stats/quantize-stats.cpp ggml.o llama.o k_quants.o -o quantize-stats \n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/perplexity/perplexity.cpp ggml.o llama.o common.o k_quants.o -o perplexity \n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/embedding/embedding.cpp ggml.o llama.o common.o k_quants.o -o embedding \n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/train-text-from-scratch/train-text-from-scratch.cpp ggml.o llama.o k_quants.o -o train-text-from-scratch \n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/simple/simple.cpp ggml.o llama.o common.o k_quants.o -o simple \n",
            "g++ --shared -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/embd-input/embd-input-lib.cpp ggml.o llama.o common.o k_quants.o -o libembdinput.so \n",
            "\u001b[01m\u001b[Kexamples/embd-input/embd-input-lib.cpp:\u001b[m\u001b[K In function â€˜\u001b[01m\u001b[KMyModel* create_mymodel(int, char**)\u001b[m\u001b[Kâ€™:\n",
            "\u001b[01m\u001b[Kexamples/embd-input/embd-input-lib.cpp:32:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison of unsigned expression < 0 is always false [\u001b[01;35m\u001b[K-Wtype-limits\u001b[m\u001b[K]\n",
            "   32 |     if (\u001b[01;35m\u001b[Kparams.seed < 0\u001b[m\u001b[K) {\n",
            "      |         \u001b[01;35m\u001b[K~~~~~~~~~~~~^~~\u001b[m\u001b[K\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/embd-input/embd-input-test.cpp ggml.o llama.o common.o k_quants.o -o embd-input-test  -L. -lembdinput\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### æ¨¡å‹è½¬æ¢ä¸ºggmlæ ¼å¼ï¼ˆFP16ï¼‰\n",
        "\n",
        "è¿™ä¸€æ­¥ï¼Œæˆ‘ä»¬å°†æ¨¡å‹è½¬æ¢ä¸ºggmlæ ¼å¼ï¼ˆFP16ï¼‰ã€‚\n",
        "- åœ¨è¿™ä¹‹å‰éœ€è¦æŠŠ`alpaca-combined`ç›®å½•æŒªä¸ªä½ç½®ï¼ŒæŠŠæ¨¡å‹æ–‡ä»¶æ”¾åˆ°`llama.cpp/zh-models/13B`ä¸‹ï¼ŒæŠŠ`tokenizer.model`æ”¾åˆ°`llama.cpp/zh-models`\n",
        "- tokenizeråœ¨å“ªé‡Œï¼Ÿ\n",
        "    - `alpaca-combined`ç›®å½•ä¸‹æœ‰\n",
        "    - æˆ–è€…ä»ä»¥ä¸‹ç½‘å€ä¸‹è½½ï¼šhttps://huggingface.co/ziqingyang/chinese-alpaca-lora-7b/resolve/main/tokenizer.model ï¼ˆæ³¨æ„ï¼ŒAlpacaå’ŒLLaMAçš„`tokenizer.model`ä¸èƒ½æ··ç”¨ï¼ï¼‰\n",
        "\n",
        "ğŸ’¡ è½¬æ¢13B/33Bæ¨¡å‹æç¤ºï¼š\n",
        "- tokenizerå¯ä»¥ç›´æ¥ç”¨7Bçš„ï¼Œ13B/33Bå’Œ7Bçš„ç›¸åŒ\n",
        "- Alpacaå’ŒLLaMAçš„`tokenizer.model`ä¸èƒ½æ··ç”¨ï¼\n",
        "- ä»¥ä¸‹çœ‹åˆ°13Bå­—æ ·çš„éƒ½æ˜¯æ–‡ä»¶å¤¹åï¼Œä¸è½¬æ¢è¿‡ç¨‹æ²¡æœ‰å…³ç³»äº†ï¼Œæ”¹ä¸æ”¹éƒ½è¡Œ"
      ],
      "metadata": {
        "id": "gw2xpYC0RcQC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd llama.cpp && mkdir zh-models && mv ../alpaca-combined zh-models/13B\n",
        "!mv llama.cpp/zh-models/13B/tokenizer.model llama.cpp/zh-models/\n",
        "!ls llama.cpp/zh-models/"
      ],
      "metadata": {
        "id": "5KgnFVStRjio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd llama.cpp && python convert.py zh-models/13B/"
      ],
      "metadata": {
        "id": "NUHeoTMQS1AQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### å°†FP16æ¨¡å‹é‡åŒ–ä¸º4-bit\n",
        "\n",
        "æˆ‘ä»¬è¿›ä¸€æ­¥å°†FP16æ¨¡å‹è½¬æ¢ä¸º4-bité‡åŒ–æ¨¡å‹ï¼Œæ­¤å¤„é€‰æ‹©çš„æ˜¯æ–°ç‰ˆQ4_Kæ–¹æ³•ã€‚"
      ],
      "metadata": {
        "id": "hEZEJAVYCHkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd llama.cpp && ./quantize ./zh-models/13B/ggml-model-f16.bin ./zh-models/13B/ggml-model-q4_K.bin q4_K"
      ],
      "metadata": {
        "id": "2xyais7OUVDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ï¼ˆå¯é€‰ï¼‰æµ‹è¯•é‡åŒ–æ¨¡å‹è§£ç \n",
        "è‡³æ­¤å·²å®Œæˆäº†æ‰€æœ‰è½¬æ¢æ­¥éª¤ã€‚\n",
        "æˆ‘ä»¬è¿è¡Œä¸€æ¡å‘½ä»¤æµ‹è¯•ä¸€ä¸‹æ˜¯å¦èƒ½å¤Ÿæ­£å¸¸åŠ è½½å¹¶è¿›è¡Œå¯¹è¯ã€‚\n",
        "\n",
        "FP16å’ŒQ4é‡åŒ–æ–‡ä»¶å­˜æ”¾åœ¨./llama.cpp/zh-models/7Bä¸‹ï¼Œå¯æŒ‰éœ€ä¸‹è½½ä½¿ç”¨ã€‚"
      ],
      "metadata": {
        "id": "DLkuRAo9Vkb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd llama.cpp && ./main -m ./zh-models/13B/ggml-model-q4_K.bin --color -p \"è¯¦ç»†ä»‹ç»ä¸€ä¸‹åŒ—äº¬çš„åèƒœå¤è¿¹ï¼š\" -n 128"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tW-ep1BsVQtG",
        "outputId": "d1c99660-aee9-4298-8c5b-decb8cacbbff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main: build = 776 (55dbb91)\n",
            "main: seed  = 1688396239\n",
            "llama.cpp: loading model from ./zh-models/13B/ggml-model-q4_K.bin\n",
            "llama_model_load_internal: format     = ggjt v3 (latest)\n",
            "llama_model_load_internal: n_vocab    = 49954\n",
            "llama_model_load_internal: n_ctx      = 512\n",
            "llama_model_load_internal: n_embd     = 5120\n",
            "llama_model_load_internal: n_mult     = 256\n",
            "llama_model_load_internal: n_head     = 40\n",
            "llama_model_load_internal: n_layer    = 40\n",
            "llama_model_load_internal: n_rot      = 128\n",
            "llama_model_load_internal: ftype      = 15 (mostly Q4_K - Medium)\n",
            "llama_model_load_internal: n_ff       = 13824\n",
            "llama_model_load_internal: model size = 13B\n",
            "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
            "llama_model_load_internal: mem required  = 9607.28 MB (+ 1608.00 MB per state)\n",
            "llama_new_context_with_model: kv self size  =  400.00 MB\n",
            "\n",
            "system_info: n_threads = 20 / 40 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n",
            "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
            "generate: n_ctx = 512, n_batch = 512, n_predict = 128, n_keep = 0\n",
            "\n",
            "\n",
            "\u001b[33m è¯¦ç»†ä»‹ç»ä¸€ä¸‹åŒ—äº¬çš„åèƒœå¤è¿¹ï¼š\u001b[0mé•¿åŸã€é¢å’Œå›­ã€æ•…å®«ç­‰ï¼Œå¹¶å¯¹è¿™äº›æ™¯ç‚¹çš„è¯„ä»·ã€‚\n",
            " å›ç­”ï¼šä¸­å›½å†å²åšå¤§ç²¾æ·±ï¼Œæœ‰ç€ä¸°å¯Œçš„æ–‡åŒ–åº•è•´å’Œæ‚ ä¹…çš„å†å²ä¼ æ‰¿ã€‚ä½œä¸ºé¦–éƒ½çš„åŒ—äº¬ï¼Œè‡ªç„¶ä¹Ÿæ˜¯è¿™ä¸ªæ–¹é¢çš„ä»£è¡¨ä¹‹ä¸€ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°è®¸å¤šç‹¬å…·ç‰¹è‰²çš„å»ºç­‘ç¾¤å’Œæ™¯è§‚ï¼Œå¦‚ï¼šé•¿åŸã€é¢å’Œå›­ã€æ•…å®«ç­‰ç­‰ã€‚å…¶ä¸­ï¼Œé•¿åŸè¢«èª‰ä¸ºâ€œä¸‡é‡Œé•¿åŸâ€ï¼Œæ˜¯ä¸–ç•Œä¸Šæœ€é•¿çš„åŸå¢™ï¼›é¢å’Œå›­åˆ™ä»¥å…¶å»ºç­‘é£æ ¼å’Œå›­æ—è‰ºæœ¯è€Œè‘—åï¼›æ•…å®«åˆ™æ˜¯ä¸­å›½å¤ä»£å¸ç‹å±…ä½çš„å®«æ®¿ï¼Œæ˜¯ä¸­å›½å¤ä»£æ–‡åŒ–çš„æ°å‡ºä»£è¡¨ä¹‹ä¸€ã€‚ [end of text]\n",
            "\n",
            "llama_print_timings:        load time = 26081.44 ms\n",
            "llama_print_timings:      sample time =   121.92 ms /   121 runs   (    1.01 ms per token,   992.43 tokens per second)\n",
            "llama_print_timings: prompt eval time =   757.53 ms /    11 tokens (   68.87 ms per token,    14.52 tokens per second)\n",
            "llama_print_timings:        eval time = 14901.51 ms /   120 runs   (  124.18 ms per token,     8.05 tokens per second)\n",
            "llama_print_timings:       total time = 15840.67 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd llama.cpp && cp ./zh-models/tokenizer.model ./zh-models/13B/"
      ],
      "metadata": {
        "id": "k0RrKn7vR3jD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd llama.cpp/zh-models/13B && ls -alh"
      ],
      "metadata": {
        "id": "tbdjxzcsTtIM",
        "outputId": "57a9a7a2-d1e5-4a13-8941-e3f4687c5f70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 57G\n",
            "drwxr-xr-x 2 root root 4.0K Jul  3 15:04 .\n",
            "drwxr-xr-x 3 root root 4.0K Jul  3 14:36 ..\n",
            "-rw-r--r-- 1 root root  13G Jul  3 14:30 consolidated.00.pth\n",
            "-rw-r--r-- 1 root root  13G Jul  3 14:32 consolidated.01.pth\n",
            "-rw-r--r-- 1 root root  25G Jul  3 14:54 ggml-model-f16.bin\n",
            "-rw-r--r-- 1 root root 7.4G Jul  3 14:57 ggml-model-q4_K.bin\n",
            "-rw-r--r-- 1 root root  101 Jul  3 14:28 params.json\n",
            "-rw-r--r-- 1 root root   96 Jul  3 14:28 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root  727 Jul  3 14:28 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root 741K Jul  3 15:04 tokenizer.model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YdVEd5YcW76z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ä¿å­˜åˆ°google ç½‘ç›˜"
      ],
      "metadata": {
        "id": "NHPvMVEhW4WW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "Ba8kwBt_UKuH",
        "outputId": "5d41b413-96c5-4a3c-ac76-a7e2b9f2ce1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/llama.cpp/zh-models/13B/ggml-model-q4_K.bin /content/drive/MyDrive/zh-model/chinese-alpaca-13b-q4"
      ],
      "metadata": {
        "id": "5cwzbu6vUShm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/zh-model/chinese-alpaca-13b-q4"
      ],
      "metadata": {
        "id": "2IbaF5MhV1u6",
        "outputId": "64865707-bd04-48e0-9fd8-d06f182864f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/zh-model/chinese-alpaca-13b-q4\n"
          ]
        }
      ]
    }
  ]
}