{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/erberry/ThinkML/blob/main/convert_and_quantize_chinese_llama_and_alpaca.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "FekGsgTJHCeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 转换并量化中文LLaMA和Alpaca模型\n",
        "\n",
        "项目地址：https://github.com/ymcui/Chinese-LLaMA-Alpaca\n",
        "\n",
        "⚠️ 内存消耗提示（确保刷出来的机器RAM大于以下要求）：\n",
        "- 7B模型：15G+\n",
        "- 13B模型：18G+\n",
        "- 33B模型：22G+\n",
        "\n",
        "💡 提示和小窍门：\n",
        "- 免费用户默认的内存只有12G左右，不足以转换模型。**实测选择TPU的话有机会随机出35G内存**，建议多试几次\n",
        "- Pro(+)用户请选择 “代码执行程序” -> “更改运行时类型” -> “高RAM”\n",
        "- 程序莫名崩掉或断开连接就说明内存爆了\n",
        "- 如果选了“高RAM”之后内存还是不够大的话，选择以下操作，有的时候会分配出很高内存的机器，祝你好运😄！\n",
        "    - 可以把GPU或者TPU也选上（虽然不会用到）\n",
        "    - 选GPU时，Pro(+)用户可选“A100”类型GPU\n",
        "\n",
        "*温馨提示：用完之后注意断开运行时，选择满足要求的最低配置即可，避免不必要的计算单元消耗（Pro只给100个计算单元）。*"
      ],
      "metadata": {
        "id": "B1c96_k3MahN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 安装相关依赖"
      ],
      "metadata": {
        "id": "vScqHD_jMFOV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5WKFJXIL6ZU"
      },
      "outputs": [],
      "source": [
        "!pip install torch==1.13.1\n",
        "!pip install transformers==4.30.2\n",
        "!pip install peft==0.3.0\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 克隆目录和代码"
      ],
      "metadata": {
        "id": "ygb1xFIMNQKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ymcui/Chinese-LLaMA-Alpaca\n",
        "!git clone https://github.com/ggerganov/llama.cpp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCEJh7NJNXz9",
        "outputId": "abc18bc9-82be-4348-9bb1-36258ee40bf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Chinese-LLaMA-Alpaca'...\n",
            "remote: Enumerating objects: 1927, done.\u001b[K\n",
            "remote: Counting objects: 100% (496/496), done.\u001b[K\n",
            "remote: Compressing objects: 100% (182/182), done.\u001b[K\n",
            "remote: Total 1927 (delta 340), reused 416 (delta 314), pack-reused 1431\u001b[K\n",
            "Receiving objects: 100% (1927/1927), 22.93 MiB | 30.25 MiB/s, done.\n",
            "Resolving deltas: 100% (1159/1159), done.\n",
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 4187, done.\u001b[K\n",
            "remote: Counting objects: 100% (1772/1772), done.\u001b[K\n",
            "remote: Compressing objects: 100% (192/192), done.\u001b[K\n",
            "remote: Total 4187 (delta 1677), reused 1599 (delta 1580), pack-reused 2415\u001b[K\n",
            "Receiving objects: 100% (4187/4187), 3.63 MiB | 23.07 MiB/s, done.\n",
            "Resolving deltas: 100% (2833/2833), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 合并模型（以Alpaca-13B为例）\n",
        "\n",
        "此处使用的是🤗模型库中提供的基模型（已是HF格式），而不是Facebook官方的LLaMA模型，因此略去将原版LLaMA转换为HF格式的步骤。\n",
        "**这里直接运行第二步：合并LoRA权重**，生成全量模型权重。可以直接指定🤗模型库的地址，也可以是本地存放地址。\n",
        "- 基模型：`elinas/llama-13b-hf-transformers-4.29` *（use at your own risk，我们比对过SHA256和正版一致，但你应确保自己有权使用该模型）*\n",
        "- LoRA模型：`ziqingyang/chinese-alpaca-lora-13b`\n",
        "   - 如果是Alpaca-Plus模型，记得要同时传入llama和alpaca的lora，教程：[这里](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/手动模型合并与转换#多lora权重合并适用于chinese-alpaca-plus)\n",
        "- 输出格式：可选pth或者huggingface，这里选择pth，因为后面要用llama.cpp量化\n",
        "\n",
        "由于要下载模型，所以需要耐心等待一下，尤其是33B模型。\n",
        "转换好的模型存放在`alpaca-combined`目录。\n",
        "如果你不需要量化模型，那么到这一步就结束了，可自行下载或者转存到Google Drive。"
      ],
      "metadata": {
        "id": "nIyxX0DSNsgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python ./Chinese-LLaMA-Alpaca/scripts/merge_llama_with_chinese_lora_low_mem.py \\\n",
        "    --base_model 'elinas/llama-13b-hf-transformers-4.29' \\\n",
        "    --lora_model 'ziqingyang/chinese-alpaca-lora-13b' \\\n",
        "    --output_type pth \\\n",
        "    --output_dir alpaca-combined"
      ],
      "metadata": {
        "id": "5AV4EW5hNhVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 比对SHA256\n",
        "\n",
        "完整值：https://github.com/ymcui/Chinese-LLaMA-Alpaca/blob/main/SHA256.md\n",
        "\n",
        "其中本示例生成的Alpaca-13B的标准SHA256：\n",
        "- 30cefb5be9091c3e17fbba5d91bf16266a2ddf86cde53370a9982b232ff8a2f4\n",
        "- ce946742b0f122f472e192c3f77d506e0c26578b4b881d07d919553333affecd\n",
        "\n",
        "使用下述命令评测后发现两者相同，合并无误。"
      ],
      "metadata": {
        "id": "iO6f_kZOPB_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sha256sum alpaca-combined/consolidated.*.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5u4QDNZPYI_",
        "outputId": "fa94adca-0363-46a7-8879-908272d7582d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30cefb5be9091c3e17fbba5d91bf16266a2ddf86cde53370a9982b232ff8a2f4  alpaca-combined/consolidated.00.pth\n",
            "ce946742b0f122f472e192c3f77d506e0c26578b4b881d07d919553333affecd  alpaca-combined/consolidated.01.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 量化模型\n",
        "接下来我们使用[llama.cpp](https://github.com/ggerganov/llama.cpp)工具对上一步生成的全量版本权重进行转换，生成4-bit量化模型。\n",
        "\n",
        "### 编译工具\n",
        "\n",
        "首先对llama.cpp工具进行编译。"
      ],
      "metadata": {
        "id": "ueexcKo-Q_EW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd llama.cpp && make"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GbjsT2wRRCR",
        "outputId": "42bd07bc-9db7-4547-e176-b9cb4e9dca5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I llama.cpp build info: \n",
            "I UNAME_S:  Linux\n",
            "I UNAME_P:  x86_64\n",
            "I UNAME_M:  x86_64\n",
            "I CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS\n",
            "I CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS\n",
            "I LDFLAGS:  \n",
            "I CC:       cc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\n",
            "I CXX:      g++ (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\n",
            "\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -c llama.cpp -o llama.o\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/main/main.cpp ggml.o llama.o common.o k_quants.o -o main \n",
            "\n",
            "====  Run ./main -h for help.  ====\n",
            "\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/quantize/quantize.cpp ggml.o llama.o k_quants.o -o quantize \n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/quantize-stats/quantize-stats.cpp ggml.o llama.o k_quants.o -o quantize-stats \n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/perplexity/perplexity.cpp ggml.o llama.o common.o k_quants.o -o perplexity \n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/embedding/embedding.cpp ggml.o llama.o common.o k_quants.o -o embedding \n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/train-text-from-scratch/train-text-from-scratch.cpp ggml.o llama.o k_quants.o -o train-text-from-scratch \n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/simple/simple.cpp ggml.o llama.o common.o k_quants.o -o simple \n",
            "g++ --shared -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/embd-input/embd-input-lib.cpp ggml.o llama.o common.o k_quants.o -o libembdinput.so \n",
            "\u001b[01m\u001b[Kexamples/embd-input/embd-input-lib.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KMyModel* create_mymodel(int, char**)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kexamples/embd-input/embd-input-lib.cpp:32:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison of unsigned expression < 0 is always false [\u001b[01;35m\u001b[K-Wtype-limits\u001b[m\u001b[K]\n",
            "   32 |     if (\u001b[01;35m\u001b[Kparams.seed < 0\u001b[m\u001b[K) {\n",
            "      |         \u001b[01;35m\u001b[K~~~~~~~~~~~~^~~\u001b[m\u001b[K\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/embd-input/embd-input-test.cpp ggml.o llama.o common.o k_quants.o -o embd-input-test  -L. -lembdinput\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 模型转换为ggml格式（FP16）\n",
        "\n",
        "这一步，我们将模型转换为ggml格式（FP16）。\n",
        "- 在这之前需要把`alpaca-combined`目录挪个位置，把模型文件放到`llama.cpp/zh-models/13B`下，把`tokenizer.model`放到`llama.cpp/zh-models`\n",
        "- tokenizer在哪里？\n",
        "    - `alpaca-combined`目录下有\n",
        "    - 或者从以下网址下载：https://huggingface.co/ziqingyang/chinese-alpaca-lora-7b/resolve/main/tokenizer.model （注意，Alpaca和LLaMA的`tokenizer.model`不能混用！）\n",
        "\n",
        "💡 转换13B/33B模型提示：\n",
        "- tokenizer可以直接用7B的，13B/33B和7B的相同\n",
        "- Alpaca和LLaMA的`tokenizer.model`不能混用！\n",
        "- 以下看到13B字样的都是文件夹名，与转换过程没有关系了，改不改都行"
      ],
      "metadata": {
        "id": "gw2xpYC0RcQC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd llama.cpp && mkdir zh-models && mv ../alpaca-combined zh-models/13B\n",
        "!mv llama.cpp/zh-models/13B/tokenizer.model llama.cpp/zh-models/\n",
        "!ls llama.cpp/zh-models/"
      ],
      "metadata": {
        "id": "5KgnFVStRjio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd llama.cpp && python convert.py zh-models/13B/"
      ],
      "metadata": {
        "id": "NUHeoTMQS1AQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 将FP16模型量化为4-bit\n",
        "\n",
        "我们进一步将FP16模型转换为4-bit量化模型，此处选择的是新版Q4_K方法。"
      ],
      "metadata": {
        "id": "hEZEJAVYCHkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd llama.cpp && ./quantize ./zh-models/13B/ggml-model-f16.bin ./zh-models/13B/ggml-model-q4_K.bin q4_K"
      ],
      "metadata": {
        "id": "2xyais7OUVDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### （可选）测试量化模型解码\n",
        "至此已完成了所有转换步骤。\n",
        "我们运行一条命令测试一下是否能够正常加载并进行对话。\n",
        "\n",
        "FP16和Q4量化文件存放在./llama.cpp/zh-models/7B下，可按需下载使用。"
      ],
      "metadata": {
        "id": "DLkuRAo9Vkb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd llama.cpp && ./main -m ./zh-models/13B/ggml-model-q4_K.bin --color -p \"详细介绍一下北京的名胜古迹：\" -n 128"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tW-ep1BsVQtG",
        "outputId": "d1c99660-aee9-4298-8c5b-decb8cacbbff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main: build = 776 (55dbb91)\n",
            "main: seed  = 1688396239\n",
            "llama.cpp: loading model from ./zh-models/13B/ggml-model-q4_K.bin\n",
            "llama_model_load_internal: format     = ggjt v3 (latest)\n",
            "llama_model_load_internal: n_vocab    = 49954\n",
            "llama_model_load_internal: n_ctx      = 512\n",
            "llama_model_load_internal: n_embd     = 5120\n",
            "llama_model_load_internal: n_mult     = 256\n",
            "llama_model_load_internal: n_head     = 40\n",
            "llama_model_load_internal: n_layer    = 40\n",
            "llama_model_load_internal: n_rot      = 128\n",
            "llama_model_load_internal: ftype      = 15 (mostly Q4_K - Medium)\n",
            "llama_model_load_internal: n_ff       = 13824\n",
            "llama_model_load_internal: model size = 13B\n",
            "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
            "llama_model_load_internal: mem required  = 9607.28 MB (+ 1608.00 MB per state)\n",
            "llama_new_context_with_model: kv self size  =  400.00 MB\n",
            "\n",
            "system_info: n_threads = 20 / 40 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n",
            "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
            "generate: n_ctx = 512, n_batch = 512, n_predict = 128, n_keep = 0\n",
            "\n",
            "\n",
            "\u001b[33m 详细介绍一下北京的名胜古迹：\u001b[0m长城、颐和园、故宫等，并对这些景点的评价。\n",
            " 回答：中国历史博大精深，有着丰富的文化底蕴和悠久的历史传承。作为首都的北京，自然也是这个方面的代表之一。在这里，我们可以看到许多独具特色的建筑群和景观，如：长城、颐和园、故宫等等。其中，长城被誉为“万里长城”，是世界上最长的城墙；颐和园则以其建筑风格和园林艺术而著名；故宫则是中国古代帝王居住的宫殿，是中国古代文化的杰出代表之一。 [end of text]\n",
            "\n",
            "llama_print_timings:        load time = 26081.44 ms\n",
            "llama_print_timings:      sample time =   121.92 ms /   121 runs   (    1.01 ms per token,   992.43 tokens per second)\n",
            "llama_print_timings: prompt eval time =   757.53 ms /    11 tokens (   68.87 ms per token,    14.52 tokens per second)\n",
            "llama_print_timings:        eval time = 14901.51 ms /   120 runs   (  124.18 ms per token,     8.05 tokens per second)\n",
            "llama_print_timings:       total time = 15840.67 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd llama.cpp && cp ./zh-models/tokenizer.model ./zh-models/13B/"
      ],
      "metadata": {
        "id": "k0RrKn7vR3jD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd llama.cpp/zh-models/13B && ls -alh"
      ],
      "metadata": {
        "id": "tbdjxzcsTtIM",
        "outputId": "57a9a7a2-d1e5-4a13-8941-e3f4687c5f70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 57G\n",
            "drwxr-xr-x 2 root root 4.0K Jul  3 15:04 .\n",
            "drwxr-xr-x 3 root root 4.0K Jul  3 14:36 ..\n",
            "-rw-r--r-- 1 root root  13G Jul  3 14:30 consolidated.00.pth\n",
            "-rw-r--r-- 1 root root  13G Jul  3 14:32 consolidated.01.pth\n",
            "-rw-r--r-- 1 root root  25G Jul  3 14:54 ggml-model-f16.bin\n",
            "-rw-r--r-- 1 root root 7.4G Jul  3 14:57 ggml-model-q4_K.bin\n",
            "-rw-r--r-- 1 root root  101 Jul  3 14:28 params.json\n",
            "-rw-r--r-- 1 root root   96 Jul  3 14:28 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root  727 Jul  3 14:28 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root 741K Jul  3 15:04 tokenizer.model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YdVEd5YcW76z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "保存到google 网盘"
      ],
      "metadata": {
        "id": "NHPvMVEhW4WW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "Ba8kwBt_UKuH",
        "outputId": "5d41b413-96c5-4a3c-ac76-a7e2b9f2ce1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/llama.cpp/zh-models/13B/ggml-model-q4_K.bin /content/drive/MyDrive/zh-model/chinese-alpaca-13b-q4"
      ],
      "metadata": {
        "id": "5cwzbu6vUShm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/zh-model/chinese-alpaca-13b-q4"
      ],
      "metadata": {
        "id": "2IbaF5MhV1u6",
        "outputId": "64865707-bd04-48e0-9fd8-d06f182864f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/zh-model/chinese-alpaca-13b-q4\n"
          ]
        }
      ]
    }
  ]
}