{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/erberry/ThinkML/blob/main/%E8%BF%90%E8%A1%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B_chatglm2_6b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFfIPDPEpz_v"
      },
      "source": [
        "### 在 GPU 上推理"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lC9M9RIar0Vo"
      },
      "source": [
        "#### 查看当前 gpu 信息\n",
        "\n",
        "colab 的 T4 GPU可以免费使用，显存 16GB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsKmy-WQrRX2",
        "outputId": "895675eb-7bc7-462c-acf1-1276ec31d8a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wed Jul  5 06:11:14 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgYbjTKBr4W-"
      },
      "source": [
        "#### 下载 ChatGLM2-6B 代码，并安装依赖"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHe2O0pzqDoc",
        "outputId": "30f9007c-0fb6-4b15-96f6-25c6a01fc16b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'ChatGLM2-6B'...\n",
            "remote: Enumerating objects: 175, done.\u001b[K\n",
            "remote: Counting objects: 100% (94/94), done.\u001b[K\n",
            "remote: Compressing objects: 100% (46/46), done.\u001b[K\n",
            "remote: Total 175 (delta 64), reused 62 (delta 48), pack-reused 81\u001b[K\n",
            "Receiving objects: 100% (175/175), 5.09 MiB | 33.02 MiB/s, done.\n",
            "Resolving deltas: 100% (88/88), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/THUDM/ChatGLM2-6B.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1xEwuayqqhG"
      },
      "outputs": [],
      "source": [
        "!cd ChatGLM2-6B && pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SBkulOxr_3k"
      },
      "source": [
        "#### 推理\n",
        "\n",
        "首次运行需要下载模型文件（大约13GB），耐心等待\n",
        "\n",
        "如果显存有限，可以修改 cli_demo.py 文件，使用量化模型进行推理：\n",
        "\n",
        "将\n",
        "\n",
        "```\n",
        "model = AutoModel.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True).cuda()\n",
        "```\n",
        "修改为\n",
        "\n",
        "```\n",
        "model = AutoModel.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True).quantize(8).cuda()\n",
        "```\n",
        "或者\n",
        "\n",
        "```\n",
        "model = AutoModel.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True).quantize(4).cuda()\n",
        "```\n",
        "\n",
        "量化模型将模型参数精度降低，以减少模型对内存的占用，但也会会降低推理准确度。\n",
        "\n",
        "如果没有显卡，可以使用 chatglm.cpp 在CPU上推理，但是推理速度会比使用GPU慢很多。 在CPU上推理见下方 【使用 chatglm.cpp 在CPU上推理】\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "epacu3-Irdov",
        "outputId": "3f9f2ad6-23d2-4656-d37b-0cd8e77ea928"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-07-05 06:23:16.192149: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Loading checkpoint shards: 100% 7/7 [01:08<00:00,  9.84s/it]\n",
            "欢迎使用 ChatGLM2-6B 模型，输入内容即可进行对话，clear 清空对话历史，stop 终止程序\n",
            "\n",
            "用户：描述蓝色\n",
            "\n",
            "ChatGLM：蓝色是一种基本颜色,通常被认为是一种平静、冷静和信任感和平静的颜色。蓝色也代表着许多不同的情感,例如天空、海洋、湖泊、蓝色代表信任、稳定、成熟和宁静等。在不同的文化和传统中,蓝色也具有不同的象征意义。\n",
            "\n",
            "用户：翻译这句话 “你好，请问天安门怎么走？“ 到英文\n",
            "\n",
            "ChatGLM：\"你好,请问天安门怎么走?\" 的英文翻译是:\"Hello, how do you go to Tiananmen Square?\"\n",
            "\n",
            "用户：写一首关于爱得诗  \n",
            "\n",
            "ChatGLM：爱得诗\n",
            "\n",
            "爱,是生命中最美好的情感\n",
            "它能让我们心灵颤抖\n",
            "让我们感到幸福和充实\n",
            "它能让我们看到世界的美\n",
            "\n",
            "爱,像春风拂面一样温暖\n",
            "它能抚摸我们的灵魂\n",
            "让我们感受到生命的意义\n",
            "它能让我们看到未来的光明\n",
            "\n",
            "爱,像一片湖水一样平静\n",
            "它能让我们心灵宁静\n",
            "让我们感受到内心的力量\n",
            "它能让我们看到生命的真谛\n",
            "\n",
            "爱,像一朵鲜花一样美丽\n",
            "它能让我们感受到生命的美好\n",
            "它能让我们感受到生命的温暖\n",
            "它能让我们感受到生命的意义\n",
            "\n",
            "爱,是生命中最美好的情感\n",
            "它能让我们心灵颤抖\n",
            "让我们感到幸福和充实\n",
            "它能让我们看到世界的美\n",
            "\n",
            "用户：解释重力得概念\n",
            "\n",
            "ChatGLM：重力得概念是指物体由于地球引力的作用而受到的力。根据牛顿第二定律,物体的加速度与作用在物体上的力成正比,与物体的质量成反比。因此,当一个物体在地球上时,它受到的重力作用于它的质量上,导致它产生加速度。重力的方向总是竖直向下的,因此物体会沿着重力方向上产生加速度。\n",
            "\n",
            "用户：蒙娜丽莎是谁画的\n",
            "\n",
            "ChatGLM：蒙娜丽莎(Mona Lisa)是由意大利艺术家达·芬奇(Leonardo da Vinci)于16世纪早期绘制的一幅肖像画。达·芬奇是文艺复兴时期最著名的艺术家之一,他的绘画风格和技能在他所处的时代非常受欢迎,对后世的艺术发展产生了巨大的影响。\n",
            "\n",
            "用户：对64开方\n",
            "\n",
            "ChatGLM：64开方是指64的算术平方根。一个数的算术平方根是指另一个数,它的平方等于原始的数。例如,8的算术平方根是2,因为2的平方等于8。\n",
            "\n",
            "用户：64的平方根\n",
            "\n",
            "ChatGLM：64的平方根是8。根据定义,一个数的平方根是指另一个数,它的平方等于原始的数。因此,64的平方根就是一个数,它的平方等于64。可以使用计算器或手算来找到64的平方根,结果都是8。\n",
            "\n",
            "用户：你能唱一首《我爱你中国》吗\n",
            "\n",
            "ChatGLM：我不能唱一首《我爱你中国》。作为一个计算机程序,我只能通过文字交流来表达我的信息。我不能唱歌或演奏乐器。但是,我可以通过查询网络来提供有关《我爱你中国》的信息,这是一首经典的爱国歌曲。\n",
            "\n",
            "用户：详述一下光合作用的过程\n",
            "\n",
            "ChatGLM：光合作用是植物和某些细菌、真菌等生物进行自养的重要过程。它是在光照作用下,利用光能将二氧化碳和水转化成储存能量的有机物,同时释放出氧气的过程。\n",
            "\n",
            "光合作用的过程可以分为两个阶段。光反应和暗反应。\n",
            "\n",
            "在光反应中,叶绿素吸收光能,并通过光化学反应将其转化为化学能。光反应包括两个反应:光系统 I 和光系统 II。光系统 II 在光线作用下,将水分子分解为氧气和电子,同时产生 ATP 和 NADPH,这些产物为后续暗反应提供能量和电子。光系统 I 则将光能转化为电子,继续产生 ATP 和 NADPH。\n",
            "\n",
            "在暗反应中,固定二氧化碳和水,合成有机物。暗反应包括两个步骤:固定 CO2 和还原 CO2。固定 CO2 的过程包括二氧化碳的固定和三碳化合物的还原。三碳化合物还原的过程包括两个步骤:三碳化合物还原为葡萄糖或甘油醛、丙酮酸。\n",
            "\n",
            "用户：生命的意义是什么？\n",
            "\n",
            "ChatGLM：生命的意义是一个非常抽象的概念,人们对它的看法不尽相同。对于某些人来说,生命的意义可能是实现自己的梦想,对于其他人来说,生命的意义可能是为了家人和社会做出贡献。对于一些人来说,生命的意义可能是通过宗教信仰来寻找的。生命的意义是一个非常复杂且个人化的概念,人们对它的看法不尽相同。\n",
            "\n",
            "�这些陈述中得出什么结论？\n",
            "\n",
            "ChatGLM：如果所有的猫都是动物,而且一些动物是狗,那么可以得出结论:猫和狗都是动物。这个结论可以从两个方面来理解:\n",
            "\n",
            "1. 猫和狗都是动物,它们属于不同的物种。猫是一种哺乳动物,而狗是一种哺乳动物。它们在生物学分类学中属于不同的类别。\n",
            "\n",
            "2. 猫和狗都是动物,它们具有一些共同的特征。例如,它们都能呼吸,都能消化食物,都能运动等。\n",
            "\n",
            "用户："
          ]
        }
      ],
      "source": [
        "!cd ChatGLM2-6B && python cli_demo.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4StL7U1YfnN"
      },
      "source": [
        "### 使用 chatglm.cpp 在CPU上推理\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_bdg6fgYn33"
      },
      "source": [
        "#### 下载 ChatGLM2-6B 和 chatglm.cpp 代码，并安装依赖的 python 包。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0OBqqpGFW7rz"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/THUDM/ChatGLM2-6B.git\n",
        "!cd ChatGLM2-6B && pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vYi-VWxXFKy",
        "outputId": "d11c6c08-8b5b-497f-d775-ec93e34316be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'chatglm.cpp'...\n",
            "remote: Enumerating objects: 125, done.\u001b[K\n",
            "remote: Counting objects:   2% (1/36)\u001b[K\rremote: Counting objects:   5% (2/36)\u001b[K\rremote: Counting objects:   8% (3/36)\u001b[K\rremote: Counting objects:  11% (4/36)\u001b[K\rremote: Counting objects:  13% (5/36)\u001b[K\rremote: Counting objects:  16% (6/36)\u001b[K\rremote: Counting objects:  19% (7/36)\u001b[K\rremote: Counting objects:  22% (8/36)\u001b[K\rremote: Counting objects:  25% (9/36)\u001b[K\rremote: Counting objects:  27% (10/36)\u001b[K\rremote: Counting objects:  30% (11/36)\u001b[K\rremote: Counting objects:  33% (12/36)\u001b[K\rremote: Counting objects:  36% (13/36)\u001b[K\rremote: Counting objects:  38% (14/36)\u001b[K\rremote: Counting objects:  41% (15/36)\u001b[K\rremote: Counting objects:  44% (16/36)\u001b[K\rremote: Counting objects:  47% (17/36)\u001b[K\rremote: Counting objects:  50% (18/36)\u001b[K\rremote: Counting objects:  52% (19/36)\u001b[K\rremote: Counting objects:  55% (20/36)\u001b[K\rremote: Counting objects:  58% (21/36)\u001b[K\rremote: Counting objects:  61% (22/36)\u001b[K\rremote: Counting objects:  63% (23/36)\u001b[K\rremote: Counting objects:  66% (24/36)\u001b[K\rremote: Counting objects:  69% (25/36)\u001b[K\rremote: Counting objects:  72% (26/36)\u001b[K\rremote: Counting objects:  75% (27/36)\u001b[K\rremote: Counting objects:  77% (28/36)\u001b[K\rremote: Counting objects:  80% (29/36)\u001b[K\rremote: Counting objects:  83% (30/36)\u001b[K\rremote: Counting objects:  86% (31/36)\u001b[K\rremote: Counting objects:  88% (32/36)\u001b[K\rremote: Counting objects:  91% (33/36)\u001b[K\rremote: Counting objects:  94% (34/36)\u001b[K\rremote: Counting objects:  97% (35/36)\u001b[K\rremote: Counting objects: 100% (36/36)\u001b[K\rremote: Counting objects: 100% (36/36), done.\u001b[K\n",
            "remote: Compressing objects:   4% (1/23)\u001b[K\rremote: Compressing objects:   8% (2/23)\u001b[K\rremote: Compressing objects:  13% (3/23)\u001b[K\rremote: Compressing objects:  17% (4/23)\u001b[K\rremote: Compressing objects:  21% (5/23)\u001b[K\rremote: Compressing objects:  26% (6/23)\u001b[K\rremote: Compressing objects:  30% (7/23)\u001b[K\rremote: Compressing objects:  34% (8/23)\u001b[K\rremote: Compressing objects:  39% (9/23)\u001b[K\rremote: Compressing objects:  43% (10/23)\u001b[K\rremote: Compressing objects:  47% (11/23)\u001b[K\rremote: Compressing objects:  52% (12/23)\u001b[K\rremote: Compressing objects:  56% (13/23)\u001b[K\rremote: Compressing objects:  60% (14/23)\u001b[K\rremote: Compressing objects:  65% (15/23)\u001b[K\rremote: Compressing objects:  69% (16/23)\u001b[K\rremote: Compressing objects:  73% (17/23)\u001b[K\rremote: Compressing objects:  78% (18/23)\u001b[K\rremote: Compressing objects:  82% (19/23)\u001b[K\rremote: Compressing objects:  86% (20/23)\u001b[K\rremote: Compressing objects:  91% (21/23)\u001b[K\rremote: Compressing objects:  95% (22/23)\u001b[K\rremote: Compressing objects: 100% (23/23)\u001b[K\rremote: Compressing objects: 100% (23/23), done.\u001b[K\n",
            "Receiving objects:   0% (1/125)\rReceiving objects:   1% (2/125)\rReceiving objects:   2% (3/125)\rReceiving objects:   3% (4/125)\rReceiving objects:   4% (5/125)\rReceiving objects:   5% (7/125)\rReceiving objects:   6% (8/125)\rReceiving objects:   7% (9/125)\rReceiving objects:   8% (10/125)\rReceiving objects:   9% (12/125)\rReceiving objects:  10% (13/125)\rReceiving objects:  11% (14/125)\rReceiving objects:  12% (15/125)\rReceiving objects:  13% (17/125)\rReceiving objects:  14% (18/125)\rReceiving objects:  15% (19/125)\rReceiving objects:  16% (20/125)\rReceiving objects:  17% (22/125)\rReceiving objects:  18% (23/125)\rReceiving objects:  19% (24/125)\rReceiving objects:  20% (25/125)\rReceiving objects:  21% (27/125)\rReceiving objects:  22% (28/125)\rReceiving objects:  23% (29/125)\rReceiving objects:  24% (30/125)\rReceiving objects:  25% (32/125)\rReceiving objects:  26% (33/125)\rReceiving objects:  27% (34/125)\rReceiving objects:  28% (35/125)\rReceiving objects:  29% (37/125)\rReceiving objects:  30% (38/125)\rReceiving objects:  31% (39/125)\rReceiving objects:  32% (40/125)\rReceiving objects:  33% (42/125)\rReceiving objects:  34% (43/125)\rReceiving objects:  35% (44/125)\rReceiving objects:  36% (45/125)\rReceiving objects:  37% (47/125)\rReceiving objects:  38% (48/125)\rReceiving objects:  39% (49/125)\rReceiving objects:  40% (50/125)\rReceiving objects:  41% (52/125)\rReceiving objects:  42% (53/125)\rReceiving objects:  43% (54/125)\rReceiving objects:  44% (55/125)\rReceiving objects:  45% (57/125)\rReceiving objects:  46% (58/125)\rReceiving objects:  47% (59/125)\rReceiving objects:  48% (60/125)\rReceiving objects:  49% (62/125)\rReceiving objects:  50% (63/125)\rReceiving objects:  51% (64/125)\rReceiving objects:  52% (65/125)\rReceiving objects:  53% (67/125)\rReceiving objects:  54% (68/125)\rReceiving objects:  55% (69/125)\rReceiving objects:  56% (70/125)\rReceiving objects:  57% (72/125)\rReceiving objects:  58% (73/125)\rReceiving objects:  59% (74/125)\rReceiving objects:  60% (75/125)\rReceiving objects:  61% (77/125)\rReceiving objects:  62% (78/125)\rReceiving objects:  63% (79/125)\rReceiving objects:  64% (80/125)\rremote: Total 125 (delta 22), reused 19 (delta 13), pack-reused 89\u001b[K\n",
            "Receiving objects:  65% (82/125)\rReceiving objects:  66% (83/125)\rReceiving objects:  67% (84/125)\rReceiving objects:  68% (85/125)\rReceiving objects:  69% (87/125)\rReceiving objects:  70% (88/125)\rReceiving objects:  71% (89/125)\rReceiving objects:  72% (90/125)\rReceiving objects:  73% (92/125)\rReceiving objects:  74% (93/125)\rReceiving objects:  75% (94/125)\rReceiving objects:  76% (95/125)\rReceiving objects:  77% (97/125)\rReceiving objects:  78% (98/125)\rReceiving objects:  79% (99/125)\rReceiving objects:  80% (100/125)\rReceiving objects:  81% (102/125)\rReceiving objects:  82% (103/125)\rReceiving objects:  83% (104/125)\rReceiving objects:  84% (105/125)\rReceiving objects:  85% (107/125)\rReceiving objects:  86% (108/125)\rReceiving objects:  87% (109/125)\rReceiving objects:  88% (110/125)\rReceiving objects:  89% (112/125)\rReceiving objects:  90% (113/125)\rReceiving objects:  91% (114/125)\rReceiving objects:  92% (115/125)\rReceiving objects:  93% (117/125)\rReceiving objects:  94% (118/125)\rReceiving objects:  95% (119/125)\rReceiving objects:  96% (120/125)\rReceiving objects:  97% (122/125)\rReceiving objects:  98% (123/125)\rReceiving objects:  99% (124/125)\rReceiving objects: 100% (125/125)\rReceiving objects: 100% (125/125), 783.87 KiB | 13.06 MiB/s, done.\n",
            "Resolving deltas:   0% (0/60)\rResolving deltas:   3% (2/60)\rResolving deltas:   6% (4/60)\rResolving deltas:   8% (5/60)\rResolving deltas:  13% (8/60)\rResolving deltas:  18% (11/60)\rResolving deltas:  25% (15/60)\rResolving deltas:  26% (16/60)\rResolving deltas:  31% (19/60)\rResolving deltas:  36% (22/60)\rResolving deltas:  40% (24/60)\rResolving deltas:  41% (25/60)\rResolving deltas:  43% (26/60)\rResolving deltas:  45% (27/60)\rResolving deltas:  55% (33/60)\rResolving deltas:  56% (34/60)\rResolving deltas:  58% (35/60)\rResolving deltas:  60% (36/60)\rResolving deltas:  61% (37/60)\rResolving deltas:  71% (43/60)\rResolving deltas:  81% (49/60)\rResolving deltas:  90% (54/60)\rResolving deltas:  91% (55/60)\rResolving deltas:  96% (58/60)\rResolving deltas: 100% (60/60)\rResolving deltas: 100% (60/60), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone --recursive https://github.com/li-plus/chatglm.cpp.git\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giY455v5ZQHt"
      },
      "source": [
        "#### 对原模型进行量化。可选的量化类型如下：\n",
        "\n",
        "- q4_0: 4-bit integer quantization with fp16 scales.\n",
        "- q4_1: 4-bit integer quantization with fp16 scales and minimum values.\n",
        "- q5_0: 5-bit integer quantization with fp16 scales.\n",
        "- q5_1: 5-bit integer quantization with fp16 scales and minimum values.\n",
        "- q8_0: 8-bit integer quantization with fp16 scales.\n",
        "- f16: half precision floating point weights without quantization.\n",
        "- f32: single precision floating point weights without quantization.\n",
        "\n",
        "以上类型代表了模型参数的精度，精度越高，模型推理所需要的内存空间越大，推理越准确。反之降低精度可以降低对内存大小的要求，但也会影响推理的准确性。\n",
        "\n",
        "这里选择 f16。大约需要13GB内存。\n",
        "\n",
        "由于需要下载原模型（大约16GB），执行比较慢。耐心等待！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFeCKG5DXVUe"
      },
      "outputs": [],
      "source": [
        "!cd chatglm.cpp && python3 convert.py -i THUDM/chatglm2-6b -t f16 -o chatglm2-ggml-f16.bin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5Rh0gaKozWU"
      },
      "source": [
        "#### 编译 chatglm.cpp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sogv8N0yXemC"
      },
      "outputs": [],
      "source": [
        "!cd chatglm.cpp && cmake -B build && cmake --build build -j"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2GsjWSZnXEA"
      },
      "source": [
        "#### 开始推理。\n",
        "\n",
        "-i 参数开启交互模式，其他参数 -h 查看"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5VujDhWYBwT",
        "outputId": "0a18f792-3d90-438d-8349-056af22f2d35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    ________          __  ________    __  ___                 \n",
            "   / ____/ /_  ____ _/ /_/ ____/ /   /  |/  /_________  ____  \n",
            "  / /   / __ \\/ __ `/ __/ / __/ /   / /|_/ // ___/ __ \\/ __ \\ \n",
            " / /___/ / / / /_/ / /_/ /_/ / /___/ /  / // /__/ /_/ / /_/ / \n",
            " \\____/_/ /_/\\__,_/\\__/\\____/_____/_/  /_(_)___/ .___/ .___/  \n",
            "                                              /_/   /_/       \n",
            "Prompt   > 你好\n",
            "ChatGLM2 > 你好👋！我是人工智能助手 ChatGLM2-6B，很高兴见到你，欢迎问我任何问题。\n",
            "Prompt   > 你能唱一首《我爱你中国》吗\n",
            "ChatGLM2 > 抱歉，作为人工智能助手，我不具有唱歌的功能。但我可以为您提供《我爱你中国》歌词。\n",
            "\n",
            "《我爱你中国》\n",
            "\n",
            "作词：乔羽\n",
            "作曲：刘炽\n",
            "\n",
            "一千里送鹅毛，\n",
            "礼送黄河岸。\n",
            "\n",
            "九百九十九道弯，\n",
            "留下丹心一片。\n",
            "\n",
            "胸怀 type-script }\n",
            "\n",
            "②怪工作效率低，\n",
            "使人累得透不过气。\n",
            "Prompt   > 详述一下光合作用的过程\n",
            "ChatGLM2 > 光合作用是植物和某些微生物利用光能将二氧化碳和水合成有机物，同时释放氧气的过程。它发生在植物细胞的叶绿体中，是一种重要的生命活动。光合作用的过程可以分为两个阶段：光反应和暗反应。\n",
            "\n",
            "光反应阶段在光照下进行，主要负责吸收光能并将其转化为化学能。该过程包括两个步骤：光系统 I 和光系统 II。\n",
            "\n",
            "1. 光系统 II：又称叶绿素光系统，负责吸收光能。它包括一个特殊的叶绿素分子，称为叶绿素 a，还有一个辅助色素——叶绿素 b。当光子照射到叶绿素 a 时，叶绿素 a 的一个电子被光子激发，形成一个高能电子。这个高能电子随后撞\n",
            "Prompt   > 生命的意义是什么？\n",
            "ChatGLM2 > 生命的意义是一个非常深刻和哲学性的问题，没办法用三言两语来回答。每个人对生命的意义都有自己的看法和定义。\n",
            "\n",
            "从生物学的角度来看，生命的意义是细胞增殖、遗传和繁衍。每个生物都有一个基本的本能，那就是生存下去并把自己的基因传递给下一代。细胞增殖、遗传和繁衍是生物存在和繁衍的基础。\n",
            "\n",
            "从哲学的角度来看，生命的意义在于追求幸福、寻找真理、实现精神上的满足。在人类中，生命的意义还在于追求自由、平等和尊严。\n",
            "\n",
            "生命的意义是一个复杂的问题，没办法用三言两语来回答。每个人都需要自己思考和寻找自己生命的意义。\n",
            "Prompt   > ^C\n"
          ]
        }
      ],
      "source": [
        "!cd chatglm.cpp && ./build/bin/main -m chatglm2-ggml-f16.bin -i"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyNkSSiFGKzi9osmho6vqs7d",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}